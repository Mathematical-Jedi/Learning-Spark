{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+--------+----------------+------------+------------+\n",
      "|index|            in_time|           out_time|berthage|         section|admin_region|parking_time|\n",
      "+-----+-------------------+-------------------+--------+----------------+------------+------------+\n",
      "|    0|2018-09-01 10:10:00|2018-09-01 12:00:00|  201091|荔园路(蛇口西段)|      南山区|        6600|\n",
      "|    1|2018-09-01 13:43:35|2018-09-01 14:29:35|  201091|荔园路(蛇口西段)|      南山区|        2760|\n",
      "|    2|2018-09-01 15:10:54|2018-09-01 16:08:54|  201091|荔园路(蛇口西段)|      南山区|        3480|\n",
      "|    3|2018-09-01 16:34:03|2018-09-01 17:56:03|  201091|荔园路(蛇口西段)|      南山区|        4920|\n",
      "|    4|2018-09-01 18:40:20|2018-09-01 20:00:20|  201091|荔园路(蛇口西段)|      南山区|        4800|\n",
      "+-----+-------------------+-------------------+--------+----------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "this_directory = Path(os.getcwd()).resolve()\n",
    "project_directory = this_directory.parent\n",
    "import sys\n",
    "sys.path.append(project_directory.as_posix())\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"test\")\\\n",
    "        .getOrCreate()\n",
    "from random import random\n",
    "from operator import add\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    " StructField(\"index\", IntegerType(), False),\n",
    " StructField(\"in_time\", TimestampType(), False),\n",
    " StructField(\"out_time\", TimestampType(), False),\n",
    " StructField(\"berthage\", IntegerType(), False),\n",
    " StructField(\"section\", StringType(), False),\n",
    " StructField(\"admin_region\", StringType(), False), \n",
    " StructField(\"parking_time\", LongType(), False),\n",
    " ])\n",
    "data_path = (project_directory/\"data/clean_parking_data.csv\").as_posix()\n",
    "df = spark.read.csv(data_path,\n",
    "                    header=True, \n",
    "                    inferSchema=True,\n",
    "                    \n",
    "                    )\n",
    "                #     schema=schema)\n",
    "df.limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|parking_time(m)|parking_time|\n",
      "+---------------+------------+\n",
      "|          110.0|        6600|\n",
      "|           46.0|        2760|\n",
      "|           58.0|        3480|\n",
      "|           82.0|        4920|\n",
      "|           80.0|        4800|\n",
      "+---------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, hour, minute, floor, explode, count, countDistinct, sum\n",
    "from pyspark.sql.types import FloatType\n",
    "# 重新计算停车时间，以分钟为单位\n",
    "parking_time = 'parking_time(m)'\n",
    "df = df.withColumn(parking_time, (col('out_time').cast('long')\n",
    "                                    - col('in_time').cast('long')) / 60)\n",
    "df.select(parking_time, 'parking_time').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|            in_time|half_hour|\n",
      "+-------------------+---------+\n",
      "|2018-09-01 10:10:00|       20|\n",
      "|2018-09-01 13:43:35|       27|\n",
      "|2018-09-01 15:10:54|       30|\n",
      "|2018-09-01 16:34:03|       33|\n",
      "|2018-09-01 18:40:20|       37|\n",
      "+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以半小时为单位划分时间区间\n",
    "df = df.withColumn('half_hour', floor((hour(col('in_time')) * 60\n",
    "                                       + minute(col('in_time'))) / 30))\n",
    "df.select('in_time', 'half_hour').show(5) # 得到了开始时间属于第几个半小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"sum(((parking_time(m) >= (half_hour * 30)) AND (parking_time(m) < ((half_hour + 1) * 30))))\" due to data type mismatch: Parameter 1 requires the \"NUMERIC\" or \"ANSI INTERVAL\" type, however \"((parking_time(m) >= (half_hour * 30)) AND (parking_time(m) < ((half_hour + 1) * 30)))\" has the type \"BOOLEAN\".;\n'Aggregate [section#649, half_hour#857L, berthage#648], [section#649, half_hour#857L, berthage#648, count(distinct berthage#648) AS N_s#901L, sum(((parking_time(m)#811 >= cast((half_hour#857L * cast(30 as bigint)) as double)) AND (parking_time(m)#811 < cast(((half_hour#857L + cast(1 as bigint)) * cast(30 as bigint)) as double)))) AS N_si#903]\n+- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, parking_time(m)#811, FLOOR((cast(((hour(in_time#646, Some(Asia/Hong_Kong)) * 60) + minute(in_time#646, Some(Asia/Hong_Kong))) as double) / cast(30 as double))) AS half_hour#857L]\n   +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, parking_time(m)#811, FLOOR((cast(((hour(in_time#646, Some(Asia/Hong_Kong)) * 60) + minute(in_time#646, Some(Asia/Hong_Kong))) as double) / cast(30 as double))) AS half_hour#834L]\n      +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#811]\n         +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#794]\n            +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#745]\n               +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#696]\n                  +- Relation [index#645,in_time#646,out_time#647,berthage#648,section#649,admin_region#650,parking_time#651] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 按照section、half_hour、berthage进行分组统计\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m agg_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mgroupBy(\u001b[39m'\u001b[39;49m\u001b[39msection\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mhalf_hour\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mberthage\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49magg(\n\u001b[1;32m      3\u001b[0m     countDistinct(\u001b[39m'\u001b[39;49m\u001b[39mberthage\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39mN_s\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m     \u001b[39msum\u001b[39;49m((col(parking_time) \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m col(\u001b[39m'\u001b[39;49m\u001b[39mhalf_hour\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m) \u001b[39m&\u001b[39;49m\n\u001b[1;32m      5\u001b[0m         (col(parking_time) \u001b[39m<\u001b[39;49m (col(\u001b[39m'\u001b[39;49m\u001b[39mhalf_hour\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m))\u001b[39m.\u001b[39;49malias(\u001b[39m'\u001b[39;49m\u001b[39mN_si\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[39m# agg_df.show(5)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.11/site-packages/pyspark/sql/group.py:175\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(c, Column) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m exprs), \u001b[39m\"\u001b[39m\u001b[39mall exprs should be Column\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     exprs \u001b[39m=\u001b[39m cast(Tuple[Column, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], exprs)\n\u001b[0;32m--> 175\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jgd\u001b[39m.\u001b[39;49magg(exprs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m_jc, _to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49m_sc, [c\u001b[39m.\u001b[39;49m_jc \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m exprs[\u001b[39m1\u001b[39;49m:]]))\n\u001b[1;32m    176\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"sum(((parking_time(m) >= (half_hour * 30)) AND (parking_time(m) < ((half_hour + 1) * 30))))\" due to data type mismatch: Parameter 1 requires the \"NUMERIC\" or \"ANSI INTERVAL\" type, however \"((parking_time(m) >= (half_hour * 30)) AND (parking_time(m) < ((half_hour + 1) * 30)))\" has the type \"BOOLEAN\".;\n'Aggregate [section#649, half_hour#857L, berthage#648], [section#649, half_hour#857L, berthage#648, count(distinct berthage#648) AS N_s#901L, sum(((parking_time(m)#811 >= cast((half_hour#857L * cast(30 as bigint)) as double)) AND (parking_time(m)#811 < cast(((half_hour#857L + cast(1 as bigint)) * cast(30 as bigint)) as double)))) AS N_si#903]\n+- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, parking_time(m)#811, FLOOR((cast(((hour(in_time#646, Some(Asia/Hong_Kong)) * 60) + minute(in_time#646, Some(Asia/Hong_Kong))) as double) / cast(30 as double))) AS half_hour#857L]\n   +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, parking_time(m)#811, FLOOR((cast(((hour(in_time#646, Some(Asia/Hong_Kong)) * 60) + minute(in_time#646, Some(Asia/Hong_Kong))) as double) / cast(30 as double))) AS half_hour#834L]\n      +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#811]\n         +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#794]\n            +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#745]\n               +- Project [index#645, in_time#646, out_time#647, berthage#648, section#649, admin_region#650, parking_time#651, (cast((cast(out_time#647 as bigint) - cast(in_time#646 as bigint)) as double) / cast(60 as double)) AS parking_time(m)#696]\n                  +- Relation [index#645,in_time#646,out_time#647,berthage#648,section#649,admin_region#650,parking_time#651] csv\n"
     ]
    }
   ],
   "source": [
    "# 按照section、half_hour、berthage进行分组统计\n",
    "agg_df = df.groupBy('section', 'half_hour', 'berthage').agg(\n",
    "    countDistinct('berthage').alias('N_s'),\n",
    "    sum((col(parking_time) >= col('half_hour') * 30) &\n",
    "        (col(parking_time) < (col('half_hour') + 1) * 30)).alias('N_si')\n",
    ")\n",
    "# agg_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 计算百分比\n",
    "agg_df = agg_df.withColumn('percentage', agg_df['N_si'] / agg_df['N_s'] * 100)\n",
    "\n",
    "# 提取时间区间的开始和结束时间\n",
    "time_df = df.select(floor((hour(col('in_time')) * 60 +\n",
    "                    minute(col('in_time'))) / 30).alias('half_hour')).distinct()\n",
    "time_df = time_df.withColumn('start_time', time_df['half_hour'] * 30)\n",
    "time_df = time_df.withColumn('end_time', (time_df['half_hour'] + 1) * 30)\n",
    "\n",
    "# 将时间区间与section进行笛卡尔积，与agg_df进行左连接，得到完整的结果\n",
    "result_df = time_df.crossJoin(df.select('section').distinct()).join(\n",
    "    agg_df, ['section', 'half_hour'], 'left')\n",
    "result_df = result_df.select('start_time', 'end_time', 'section',\n",
    "                             'N_s', 'N_si', 'percentage').orderBy('section', 'start_time')\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "input(\"按回车继续...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
