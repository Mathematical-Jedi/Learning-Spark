{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/08 23:02:17 WARN Utils: Your hostname, zylab resolves to a loopback address: 127.0.1.1; using 10.16.88.247 instead (on interface eno1)\n",
      "23/05/08 23:02:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/08 23:02:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+--------+----------------+------------+------------+\n",
      "|index|            in_time|           out_time|berthage|         section|admin_region|parking_time|\n",
      "+-----+-------------------+-------------------+--------+----------------+------------+------------+\n",
      "|    0|2018-09-01 10:10:00|2018-09-01 12:00:00|  201091|荔园路(蛇口西段)|      南山区|        6600|\n",
      "|    1|2018-09-01 13:43:35|2018-09-01 14:29:35|  201091|荔园路(蛇口西段)|      南山区|        2760|\n",
      "|    2|2018-09-01 15:10:54|2018-09-01 16:08:54|  201091|荔园路(蛇口西段)|      南山区|        3480|\n",
      "|    3|2018-09-01 16:34:03|2018-09-01 17:56:03|  201091|荔园路(蛇口西段)|      南山区|        4920|\n",
      "|    4|2018-09-01 18:40:20|2018-09-01 20:00:20|  201091|荔园路(蛇口西段)|      南山区|        4800|\n",
      "+-----+-------------------+-------------------+--------+----------------+------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yecm/anaconda3/envs/spark/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from boilerplates import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'test'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/yecm/yecanming/repo/P_CS_Labs/Distrubuted/Assignment3-Spark/src/spark-warehouse'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '10.16.88.247'),\n",
       " ('spark.app.submitTime', '1683558138258'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1683558139567'),\n",
       " ('spark.driver.port', '35223'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.startTime', '1683558138415'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                        (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written to /home/yecm/yecanming/repo/P_CS_Labs/Distrubuted/Assignment3-Spark/output/r1.csv\n"
     ]
    }
   ],
   "source": [
    "# 问题1: 对于每个section，统计不同的停车场的数量（不是统计停车记录的数量，需要Distinct）\n",
    "res1 = df.groupby(col_section).agg(count_distinct(col_berthage).alias(\"count\"))\n",
    "write_df(res1, \"r1.csv\")\n",
    "# pandas得到单个文件。如果是write函数，可能得到多个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题2\n",
    "# 找到所有unique的berthage，同时找到每个berthage对应的那一个section\n",
    "# 一个bertahge有很多条记录，每个记录都有section，我们选择哪个呢?\n",
    "# 我们可以继续分组，看看是不是一样的。\n",
    "res2 = df.select(col_berthage, col_section)\\\n",
    "    .groupby(col_berthage, col_section)\\\n",
    "    .agg(count_distinct(col_section).alias(\"count\"))\\\n",
    "    # .agg(mode('section').alias('section'))\n",
    "assert all([i['count']==1 for i in res2.toLocalIterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==>                                                     (1 + 19) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written to /home/yecm/yecanming/repo/P_CS_Labs/Distrubuted/Assignment3-Spark/output/r2.csv\n"
     ]
    }
   ],
   "source": [
    "# 符合条件，那么我们可以直接取第一个section\n",
    "res2 = df.select(col_berthage, col_section)\\\n",
    "    .groupby(col_berthage)\\\n",
    "    .agg(first(col_section).alias('section'))\n",
    "# res.show()\n",
    "write_df(res2, \"r2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2_5 = res2.groupby(col_section).agg(count(col_berthage).alias(\"count\"))\n",
    "dict(res2_5.toLocalIterator())==dict(res1.toLocalIterator()) # 说明r1和r2的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written to /home/yecm/yecanming/repo/P_CS_Labs/Distrubuted/Assignment3-Spark/output/r3.csv\n"
     ]
    }
   ],
   "source": [
    "# 问题3\n",
    "# 求每一个区域的平均停车时间，单位为取整后的秒\n",
    "res3 = df.select(col_section, col_parking_time)\\\n",
    "    .groupby(col_section)\\\n",
    "    .agg(mean(col_parking_time).cast('long').alias('avg_parking_time'))\n",
    "write_df(res3, \"r3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written to /home/yecm/yecanming/repo/P_CS_Labs/Distrubuted/Assignment3-Spark/output/r4.csv\n"
     ]
    }
   ],
   "source": [
    "# 问题4\n",
    "# 求每一个停车场的平均停车时间，单位为取整后的秒，降序排序\n",
    "res4 = df.select(col_berthage, col_parking_time)\\\n",
    "    .groupby(col_berthage)\\\n",
    "    .agg(mean(col_parking_time).cast('long').alias('avg_parking_time'))\\\n",
    "    .sort(col('avg_parking_time').desc())\n",
    "# res4.show()\n",
    "write_df(res4, \"r4.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
